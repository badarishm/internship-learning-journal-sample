#  Data Sourcing & Web Scraping â€“ Summary

This session covered different ways of collecting and extracting data from websites, documents, and APIs using both manual and automated tools.

## Key Learnings

### Data Sourcing
- Collected data using spreadsheets like Excel and Google Sheets.
- Learned basic scraping using sheet functions.

### CLI & API Tools
- Used command-line tools for crawling websites.
- Retrieved data using APIs like:
  - BBC Weather API
  - Nominatim API
  - Wikipedia API

### Web Automation
- Scraped websites like IMDb using JavaScript.
- Automated browsing tasks using Playwright.

### Document Processing
- Extracted data from PDFs using Tabula.
- Converted PDFs and HTML files into Markdown format.

### AI-Based Scraping
- Used LLMs for website scraping.
- Extracted data from video content using AI.

### Advanced Techniques
- Scheduled scraping tasks using GitHub Actions.
- Worked with real-world data sources like emarketer.

### Practical Use
- Learned how to convert raw scraped data into useful insights.

##  What I Took Away

- Data can be collected using multiple tools and methods.
- APIs provide structured and reliable data access.
- Automation makes scraping faster and scalable.
- AI can enhance scraping from complex sources.
- Scheduling helps maintain updated datasets.


